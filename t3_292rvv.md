---
kind: thread
author: want_to_want
domain: self.haskell
is_self: true
created: 1403719397
permalink: /r/haskell/comments/292rvv/learn_reasoning_about_time_and_space_complexity/
id: 292rvv
name: t3_292rvv
subreddit: haskell
subreddit_id : t5_2qh36
title: [learn] Reasoning about time and space complexity
url: https://www.reddit.com/r/haskell/comments/292rvv/learn_reasoning_about_time_and_space_complexity/
---

I'm trying to figure out what can be said mathematically about the time and space complexity of lazy programs. Here's what I know so far:

- If we look at whole programs, then the time complexity of lazy evaluation is asymptotically at least as good as eager evaluation, but space complexity might be worse.

- If we look at individual functions in a lazy language, things are less clear. Some functions, like quicksort, seem to have a well-defined time and space complexity, which is roughly "how much time and space this function would use if both the arguments and the result were fully evaluated", assuming that both are finite. But that definition is not enough to analyze some uses of these functions, like implementing quickselect in terms of quicksort. And it doesn't seem to work for functions that are intended to deal with infinite data structures, like repeat. Time is just as problematic as space in this regard.

- Okasaki's book has a nice approach to analyzing some lazy data structures, though it doesn't seem to generalize to everything you can write in a lazy language.

The question is, where should I look next? How do people analyze the time and space complexity of functions in a lazy language, both in theory and in practice?
